# ğŸ“Š Mastering Supervised Regression

ğŸš€ **End-to-End Regression Modeling Project**

This repository contains an end-to-end supervised regression pipeline, covering data preprocessing, feature engineering, model training, evaluation, and deployment. The goal is to build an optimized regression model and compare various algorithms.

---

## ğŸ“‚ Folder Structure

```
ğŸ“¦ Mastering-Supervised-Regression
â”œâ”€â”€ ğŸ“ code                # Jupyter Notebook and scripts
â”‚   â”œâ”€â”€ end-to-end-regression-modeling.ipynb
â”œâ”€â”€ ğŸ“ report              # Documentation and reports
â”‚   â”œâ”€â”€ Mastering Supervised Regression.pdf
â”œâ”€â”€ ğŸ“ model_and_results   # Saved models and evaluation results
â”‚   â”œâ”€â”€ best_model.pkl
â”‚   â”œâ”€â”€ RMSE_comparison.png
â”‚   â”œâ”€â”€ R2_comparison.png
â”œâ”€â”€ README.md              # Project overview and guide
```

---

## ğŸ“œ Project Overview

This project demonstrates the process of building and fine-tuning regression models. The dataset is processed through various stages, including: 

âœ… **Data Preprocessing** (Handling missing values, outliers, and scaling)  
âœ… **Feature Engineering** (Creating new features, encoding categorical variables)  
âœ… **Model Training & Hyperparameter Tuning** (Ridge, Lasso, SVR, Decision Trees, Random Forest, KNN)  
âœ… **Performance Evaluation** (Metrics like MSE, RMSE, RÂ², visualization of results)  
âœ… **Deployment** (Saving the model and making predictions)  

---

## ğŸ“Š Dataset & Preprocessing

- Data Cleaning: Handling missing values and outliers.  
- Feature Engineering: Encoding categorical variables, feature selection.  
- Train-Test Split: Using an 80/20 split.  

ğŸ”¹ **Example Visualization:**  
![Dataset Visualization](model_and_results/data_visualization.png)

---

## ğŸ¤– Model Training & Evaluation

### ğŸ† Trained Models:
- **Linear Regression**
- **Ridge Regression (L2 Regularization)**
- **Lasso Regression (L1 Regularization)**
- **Polynomial Regression**
- **Support Vector Regression (SVR)**
- **Decision Tree Regression**
- **Random Forest Regression**
- **K-Nearest Neighbors (KNN)**

### ğŸ“ˆ Model Performance (Evaluation Metrics):
| Model          | MSE  | RMSE | MAE  | RÂ²  |
|---------------|------|------|------|-----|
| Ridge         | 0.10 | 0.31 | 0.20 | 0.91 |
| Lasso         | 0.10 | 0.31 | 0.20 | 0.91 |
| Polynomial    | 0.15 | 0.39 | 0.29 | 0.86 |
| SVR           | 0.12 | 0.35 | 0.19 | 0.89 |
| Decision Tree | 0.15 | 0.39 | 0.28 | 0.86 |
| Random Forest | 0.08 | 0.28 | 0.19 | 0.93 |
| KNN           | 0.15 | 0.39 | 0.28 | 0.86 |

---

## ğŸ¯ Key Takeaways

âœ” **Random Forest** achieved the best performance with the lowest error and highest RÂ².  
âœ” **Ridge & Lasso** performed well, with Ridge handling multicollinearity effectively.  
âœ” **Polynomial Regression, Decision Tree, and KNN** had higher errors, indicating overfitting.  
âœ” **Feature scaling significantly impacted SVR & KNN performance.**  

---

## ğŸš€ Model Deployment

- The best model is saved as `best_model.pkl` using `joblib`.
- Example Prediction Function:

```python
from joblib import load

# Load the model
model = load("model_and_results/best_model.pkl")

# Example input (must match training feature format)
example_input = [[3, 1200, 2, 1]]

# Make prediction
predicted_value = model.predict(example_input)
print(f"Predicted Value: {predicted_value}")
```

---

## ğŸ“Œ Future Improvements

ğŸ”¹ Feature Selection using advanced techniques (e.g., SHAP values)  
ğŸ”¹ Experimenting with deep learning regression models  
ğŸ”¹ Hyperparameter tuning using Bayesian Optimization  

---

## ğŸ“œ References

- Scikit-learn Documentation  
- Kaggle Datasets  
- Matplotlib & Seaborn for Visualization  

ğŸ’¡ **Contributions & Feedback Welcome!**  

---

ğŸ“¢ _If you find this project useful, consider giving it a â­ on GitHub!_ ğŸš€
